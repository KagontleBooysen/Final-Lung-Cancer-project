{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KagontleBooysen/Final-Lung-Cancer-project/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers nltk\n",
        "\n",
        "# Importing necessary libraries\n",
        "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSSHEQ451YIb",
        "outputId": "b927cd00-f75b-4755-883c-96d547b30fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT model and tokenizer\n",
        "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EyddK6a2nrs",
        "outputId": "eb8acde6-4cbe-4c8b-a741-fabb554d690d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
            "\n",
            "All the weights of TFBertForQuestionAnswering were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"\n",
        "Lung cancer is a type of cancer that begins in the lungs, characterized by uncontrolled cell growth in the lung tissues. It is a significant health challenge in South Africa, with an estimated 8,000 new cases diagnosed annually. Lung cancer is the leading cause of cancer-related deaths among men and one of the top five cancers affecting women. The primary risk factor for lung cancer is smoking, with approximately 20% of the adult population identified as smokers, contributing to its high incidence. Environmental factors, such as exposure to asbestos and industrial pollutants, also play a role. Additionally, South Africaâ€™s high HIV/AIDS prevalence, with about 13% of the adult population living with HIV, exacerbates the lung cancer burden, as immunocompromised individuals are at higher risk. Late-stage diagnosis is common due to limited access to healthcare services and inadequate screening programs, resulting in poorer outcomes. Treatment access is further hindered by the high costs associated with chemotherapy, radiation, and surgical interventions, which are often beyond the reach of many South Africans relying on the overburdened public healthcare system. Public health efforts, including anti-smoking campaigns and initiatives for early detection, are ongoing, but there is a pressing need for more comprehensive and accessible screening programs, along with enhanced support systems for patients and their families.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1bn5LET32rIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocess_text function for text cleaning and tokenization\n",
        "def preprocess_text(text):\n",
        "    \"\"\" Clean and tokenize text. \"\"\"\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
        "    return text"
      ],
      "metadata": {
        "id": "DZIb4wr92z1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define chat function for answering questions\n",
        "def chat(question, reference=None):\n",
        "    try:\n",
        "        # Preprocess question text\n",
        "        question = preprocess_text(question)\n",
        "\n",
        "        # Tokenize the input message and context\n",
        "        inputs = tokenizer(question, context, return_tensors='tf')\n",
        "\n",
        "        # Get the model's output\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Extract the answer start and end logits\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "        # Get the most likely start and end token positions\n",
        "        start_index = tf.argmax(start_logits, axis=-1).numpy()[0]\n",
        "        end_index = tf.argmax(end_logits, axis=-1).numpy()[0]\n",
        "\n",
        "        # Check if the indices are valid\n",
        "        if start_index <= end_index and start_index < len(inputs['input_ids'][0]) and end_index < len(inputs['input_ids'][0]):\n",
        "            # Convert token indices back to tokens\n",
        "            input_ids = inputs['input_ids'].numpy()[0]\n",
        "            answer_tokens = tokenizer.convert_ids_to_tokens(input_ids[start_index:end_index+1])\n",
        "\n",
        "            # Clean the answer\n",
        "            answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "            # Calculate BLEU score if reference answer is provided\n",
        "            if reference:\n",
        "                reference = preprocess_text(reference).split()\n",
        "                candidate = preprocess_text(answer).split()\n",
        "                bleu_score = sentence_bleu([reference], candidate)\n",
        "                print(f\"Reference: {' '.join(reference)}\")\n",
        "                print(f\"Candidate: {' '.join(candidate)}\")\n",
        "                print(f\"BLEU Score: {bleu_score}\")\n",
        "                return answer, bleu_score\n",
        "            else:\n",
        "                return answer, None\n",
        "        else:\n",
        "            return \"I'm sorry, I don't have the information you are looking for.\", None\n",
        "    except Exception as e:\n",
        "        return str(e), None"
      ],
      "metadata": {
        "id": "sISvNycT22pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder function for fine-tuning the model with actual data\n",
        "def fine_tune_model(train_data, val_data):\n",
        "    \"\"\"\n",
        "    Placeholder function for fine-tuning the model.\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "\n",
        "    # Compile the model with the optimizer\n",
        "    model.compile(optimizer=optimizer, loss=model.compute_loss)\n",
        "\n",
        "    # Convert the data to TensorFlow Dataset objects\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {'input_ids': train_data['input_ids'], 'attention_mask': train_data['attention_mask']},\n",
        "        {'start_positions': train_data['start_positions'], 'end_positions': train_data['end_positions']}\n",
        "    ))\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {'input_ids': val_data['input_ids'], 'attention_mask': val_data['attention_mask']},\n",
        "        {'start_positions': val_data['start_positions'], 'end_positions': val_data['end_positions']}\n",
        "    ))"
      ],
      "metadata": {
        "id": "7xYd1VX45twM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"What is a significant health challenge in South Africa?\"\n",
        "reference = \"Lung cancer\"\n",
        "answer, bleu_score = chat(question, reference)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")\n",
        "if bleu_score is not None:\n",
        "    print(f\"BLEU Score: {bleu_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsUCTDQn29ap",
        "outputId": "637013f0-1c69-4b02-ec71-0cf96de9a582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference: lung cancer\n",
            "Candidate: lung cancer\n",
            "BLEU Score: 1.491668146240062e-154\n",
            "Question: What is a significant health challenge in South Africa?\n",
            "Answer: lung cancer\n",
            "BLEU Score: 1.491668146240062e-154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaLeSxFaLlsVTwO/i6Gz8y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}